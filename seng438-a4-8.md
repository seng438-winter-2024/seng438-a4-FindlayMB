**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group: 8        |
| --------------- |
| Jiawei He       |
| Nicholas Garcia |
| Sarah Qin       |
| Findlay Brown   |

# Introduction
The objective of this assignment is to help us get familarize with mutation testing and GUI testing tools and learn how to utilize these tools to create better test cases to increase the overall testing quality. In this assignment, we will be using PiTest to analyze the mutation scores and Selenium for GUI testing. It is important to come an understanding of what mutation testing is because it helps assessing the effectiveness of the already created test suites. 

# Analysis of 10 Mutants of the Range class

**Range.intersects(double b0, double b1)**

- Mutation: changed (b0 <= this.lower) to (b0 < this.lower) in the if statement
  - Status: SURVIVED
  - Analysis: This mutant survived because b0 values < this.lower are still valid in the b0 <= this.lower condition.
- Mutation: changed return from (b0 < this.upper && b1 >= b0) to false in the else statement, hence the tests still passed.
  - Status: KILLED
  - Analysis: This mutant was killed because changing it to always returning false causes tests to fail where the return statement was supposed to change depending on the conditions.

**Range.constrain(double value)**

- Mutation: changed (value > this.upper) to (value >= this.upper) in the second if statement
  - Status: SURVIVED
  - Analysis: This mutant survived because test values > this.upper are still valid in value >= this.upper, hence the tests still passed.
- Mutation: changed (value > this.upper) to (value <= this.upper) in the second if statement
  - Status: KILLED
  - Analysis: This mutant was killed because the condition is different, causing it to go to the other condtions with a different return value and fails the test.

**Range.expandToInclude(Range range, double value)**

- Mutation: changed (value < range.getLowerBound()) to (value <= range.getLowerBound()) in the second if statement
  - Status: SURVIVED
  - Analysis: This mutant survived because test values < range.getLowerBound() are still valid in values <= range.getLowerBound(), hence the tests still passed.
- Mutation: changed (value < range.getLowerBound()) to (value >= range.getLowerBound()) in the second if statement
  - Status: KILLED
  - Analysis: This mutant was killed because the condition is different, causing it to go to the other condtions with a different return value and fails the test.

**Range.expand(Range range, double lowerMargin, double upperMargin)**

- Mutation: changed (lower > upper) to (lower >= upper) in the if statement
  - Status: SURVIVED
  - Analysis: This mutant survived because test values lower > upper are still valid in lower >= upper, hence the tests still passed.
- Mutation: changed (lower > upper) to (lower <= upper) in the if statement
  - Status: KILLED
  - Analysis: This mutant was killed because the condition is different, causing it to go to the other condtions with a different return value and fails the test.

**Range.shiftWithNoZeroCrossing(double value, double delta)**

- Mutation: changed (value > 0.0) to (value >= 0.0) in the if statement
  - Status: SURVIVED
  - Analysis: This mutant survived because test values > 0.0 are still valid in values >= 0.0, hence the tests still passed.
- Mutation: changed (value > 0.0) to (value <= 0.0) in the if statement
  - Status: KILLED
  - Analysis: This mutant was killed because the condition is different, causing it to go to the other condtions with a different return value and fails the test.

# Report all the statistics and the mutation score for each test class

**Mutation Coverage of Range and DataUtilities Before Changes**

![alt text](/images/before_changes.png)

**Mutation Coverage of Range and DataUtilities After Changes**

![alt text](/images/after_changes.png)

# Analysis drawn on the effectiveness of each of the test classes
The test suite for the DataUtilities class proved to already exhibit a 100% mutation coverage alongside a 99% line coverage in the previous iteration of the SUT from assignment 3. This resulted in a test strength of 100% and indicates that our approach to test design for DataUtilities is effective. Our test suite for the Range class from assignment 3 appears to be slightly less effective, with a mutation coverage of 89% and test strength of 92%. After making new changes to kill mutants in our Range class test suite, we managed to increase the mutation coverage to 100%.

# A discussion on the effect of equivalent mutants on mutation score accuracy
Even though equivalent mutants can not be killed, it does not change the mutation score. However, it does affect the accuracy of the mutation score because there is technically still a mutant present but it cannot be handled thus it does not go toward the mutation score calculation. This is because an equivalent mutant does not actually change the functionality of the original code even though it is syntactically different. Thus, our test suites would not be able to kill the mutants. 

# A discussion of what could have been done to improve the mutation score of the test suites
There were a lot of equivalent mutants stemming from if statements where the condition < was changed to <= or vice versa.
Our test cases did not kill these mutants. To improve on the mutation score, we had to alter the source files. Changes 
were made to minimize the amount of equivalent mutants to the point where both class files reached a 100% mutation score
coverage.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing
Mutation testing evaluates software testing effectiveness by introducing mutations to code, helping identify test suite weaknesses, enhance test quality, and improve software reliability. <br>
**Advantages**: can be automated, its systematic, know when to stop, check which test suite is better <br>
**Disadvantages**: can not kill equivalent mutants, there may be an enormous amount of mutants

# Explain your SELENUIM test case design process
The Selenium test cases were designed based on the functionality. For the functions that requires text entry, we tested valid input, invalid input, and inputs with different data type. For example with the login function, we tested it with valid to see if it will prompt the next page and invalid input to see if it displays the incorrect password/email message. To check the functionalty of the button, we tested to see if the correct prompt pops up. An example with the submit button in the register page, the submit button was unclickable when we entered an invalid email and the next step was not prompted. 

# Explain the use of assertions and checkpoints
We implemented assertion and verification commands in our Selenium test cases to automate the step of checking the application behaviour following a series of test inputs for various functions. Each assertion/verification step is able to check the state of a specific element in the application at different points of the test, and compare it to the expected correct outcome without needing to be manually reviewed. Thus, we can effectively design our test cases around a series of "checkpoints" that perform a group of assertions on the state of multiple elements throughout different stages in the test. 

For example, in our searchCustomerService case for ebay.ca, 8 automated inputs are used to test the behavior of the support search function when presented with a valid query. The first 2 steps involve commands `open` and `click` to navigate to the support page, followed by an `assert editable` command to check that the target search bar element is editable before proceeding (serves as our first checkpoint). Steps 4-6 then search with the text "Reset Password", before finally executing 2 separate assert commands to check that the first search result is present, and also contains the text "Reset your password" (serves as our second checkpoint).

# How did you test each functionality with different test data
We tested each functionality of ebay.ca with different test data by creating multiple individual test cases for a specified function. For example, in order to test the search function for product listings using a variety of input scenarios, we designed 4 separate test cases each only differing by a single input value (search query) and the expected element state being verified: 
- `searchCategoriesInput`: The test navigates to the "Cell Phones & Accessories" category, and executes the `verify text` command to check the subsequent page's header contains the text "Cell Phones, Smart Watches & Accessories".

- `searchMultiWordInput`: The test navigates to the main search bar to search for "pair of socks", and executes the `verify text` command to check that the subsequent page's search count contains the text "130,000+ results for pair of socks".

- `searchNoInput`: The test navigates to the main search bar to search without any text input, and executes the `verify element presents` command to check that the subsequent page shows the default category listing.

- `searchOneWordInput`: The test navigates to the main search bar to search for "ball", and executes the `verify text` command to check that the subsequent page's search count contains the text "6,900,000+ results for ball".

# Discuss advantages and disadvantages of Selenium vs. Sikulix
The advantage of Selenium is that once you start recording, it will record each step and can rerun that multiple times. 
The advantage of Sikulix is that it uses the image searching to look for the component to perform its actions. We can see the component on the screen before running the test. The disadvantage of both IDE is the syntax. They use different syntax and does not provide many information on how to use it or how to start. 

# How the team work/effort was divided and managed
For part 1, the team was split into pairs to record the results of mutation testing and design new test cases. The tasks required for the Range class was assigned to Sarah-Nick, and the DataUtilities class to Findlay-Javy. We then reviewed all the tests as a group to check for inconsistencies or defects in test methodology, as well as adherance to JUnit testing standards.

For part 2, each member was responsible for designing GUI test cases for 2 different functionalities. Nick tested the listing search bar and user login, Sarah tested the shopping cart and user registration, Findlay tested the community forum and announcments page links, and Javy tested the customer support search and assistant chat.

# Difficulties encountered, challenges overcome, and lessons learned
We initially encountered some difficulty with setting up mutation testing with Pitest on our test suites in the Eclipse IDE, where an exception in the console prevented us from running the mutation tests on a new project with the system from assignment 3. We traced the issue to a project configuration file that references a static local path for the external .jar dependencies, and were able to resolve it.

The biggest challenge we overcame in this lab was designing effective automated GUI tests using Selenium. We learned the difference between the various kinds of `verify` and `assert` commands, and underwent trial and error on several target elements of the SUT (ebay.ca) to figure out the most efficient way of consistently testing each chosen functionality.

# Comments/feedback on the lab itself
The tasks in part 1 of this lab was very straightforward and its instructions were well documented. We were pleasantly surprised by how practical part 2 was given that it helped us quickly start creating an automated GUI test on a real website with just a browser extension.